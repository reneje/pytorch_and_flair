{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 21:51:16,945 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/NER-conll03-english/en-ner-conll03-v0.4.pt not found in cache, downloading to /tmp/tmpkn2l_fim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432197603/432197603 [04:26<00:00, 1622840.32B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 21:55:44,458 copying /tmp/tmpkn2l_fim to cache at /hdd1/rini.jannati/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 21:55:44,754 removing temp file /tmp/tmpkn2l_fim\n",
      "2019-07-23 21:55:44,794 loading file /hdd1/rini.jannati/.flair/models/en-ner-conll03-v0.4.pt\n",
      "Sentence: \"I love Berlin .\" - 4 Tokens\n",
      "the following NER tags are found: \n",
      "LOC-span [3]: \"Berlin\"\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "#make a sentence\n",
    "sentence = Sentence('I love Berlin .')\n",
    "\n",
    "#load the NER tagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "#run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(sentence)\n",
    "print('the following NER tags are found: ')\n",
    "\n",
    "#iterate over entities\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"The grass is green .\" - 5 Tokens\n"
     ]
    }
   ],
   "source": [
    "# Make a sentence object by passing a whitespace tokenized string\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# Print the object to see what's in there\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 4 green\n",
      "Token: 4 green\n"
     ]
    }
   ],
   "source": [
    "# using the token id\n",
    "print(sentence.get_token(4))\n",
    "# using the index itself\n",
    "print(sentence[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "Token: 2 grass\n",
      "Token: 3 is\n",
      "Token: 4 green\n",
      "Token: 5 .\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Token: 4 green\" is tagged as \"\" with confidence score \"1.0\"\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Label\n",
    "\n",
    "tag: Label = sentence[3].get_tag('ner')\n",
    "\n",
    "print(f'\"{sentence[3]}\" is tagged as \"{tag.value}\" with confidence score \"{tag.score}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = Sentence('France is the current world cup winner.')\n",
    "\n",
    "# add a label to a sentence\n",
    "sentence.add_label('sports')\n",
    "\n",
    "# a sentence can also belong to multiple classes\n",
    "sentence.add_labels(['sports', 'world cup'])\n",
    "\n",
    "# you can also set the labels while initializing the sentence\n",
    "sentence = Sentence('France is the current world cup winner.', labels=['sports', 'world cup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"France is the current world cup winner.\" - 7 Tokens - Labels: [sports (1.0), world cup (1.0)] \n",
      "sports (1.0)\n",
      "world cup (1.0)\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('France is the current world cup winner.', labels=['sports', 'world cup'])\n",
    "\n",
    "print(sentence)\n",
    "for label in sentence.labels:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 23:15:21,115 loading file /hdd1/rini.jannati/.flair/models/en-ner-conll03-v0.4.pt\n",
      "George <B-PER> Washington <E-PER> went to Washington <S-LOC> .\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load('ner')\n",
    "sentence = Sentence('George Washington went to Washington .')\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER-span [1,2]: \"George Washington\"\n",
      "LOC-span [5]: \"Washington\"\n",
      "{'text': 'George Washington went to Washington .', 'labels': [], 'entities': [{'text': 'George Washington', 'start_pos': 0, 'end_pos': 17, 'type': 'PER', 'confidence': 0.9967881441116333}, {'text': 'Washington', 'start_pos': 26, 'end_pos': 36, 'type': 'LOC', 'confidence': 0.9993712306022644}]}\n"
     ]
    }
   ],
   "source": [
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)\n",
    "    \n",
    "print(sentence.to_dict(tag_type='ner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sentence.to_dict(tag_type='ner')\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 23:19:06,800 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.2/FRAME-conll12--h256-l1-b8-%2Bnews%2Bnews-forward%2Bnews-backward--v0.2/en-frame-ontonotes-v0.2.pt not found in cache, downloading to /tmp/tmpkrud9tc0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1227395523/1524671476 [23:39<50:55:18, 1621.63B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 23:42:47,162 copying /tmp/tmpkrud9tc0 to cache at /hdd1/rini.jannati/.flair/models/en-frame-ontonotes-v0.2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 23:42:47,987 removing temp file /tmp/tmpkrud9tc0\n",
      "2019-07-23 23:42:48,098 loading file /hdd1/rini.jannati/.flair/models/en-frame-ontonotes-v0.2.pt\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "pickle data was truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7723fe836f2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequenceTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'frame'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# make German sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentence_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'George returned to Berlin to return his hat .'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/flair/nn.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, model)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# see https://github.com/zalandoresearch/flair/issues/351\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_big_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_model_with_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "tagger = SequenceTagger.load('frame')\n",
    "\n",
    "# make German sentence\n",
    "sentence_1 = Sentence('George returned to Berlin to return his hat .')\n",
    "sentence_2 = Sentence('He had a look at different hats .')\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence_1)\n",
    "tagger.predict(sentence_2)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence_1.to_tagged_string())\n",
    "print(sentence_2.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 23:45:55,477 loading file /hdd1/rini.jannati/.flair/models/en-ner-conll03-v0.4.pt\n",
      "[Sentence: \"This is a sentence .\" - 5 Tokens, Sentence: \"This is another sentence .\" - 5 Tokens, Sentence: \"I love Berlin .\" - 4 Tokens]\n"
     ]
    }
   ],
   "source": [
    "# your text of many sentences\n",
    "text = \"This is a sentence. This is another sentence. I love Berlin.\"\n",
    "\n",
    "# use a library to split into sentences\n",
    "from segtok.segmenter import split_single\n",
    "sentences = [Sentence(sent, use_tokenizer=True) for sent in split_single(text)]\n",
    "\n",
    "# predict tags for list of sentences\n",
    "tagger: SequenceTagger = SequenceTagger.load('ner')\n",
    "tagger.predict(sentences)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 23:48:30,897 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/TEXT-CLASSIFICATION_imdb/imdb.pt not found in cache, downloading to /tmp/tmpmdvfvrgv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2794252905/2794252905 [18:12<00:00, 2558450.43B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 00:06:44,289 copying /tmp/tmpmdvfvrgv to cache at /hdd1/rini.jannati/.flair/models/imdb.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 00:06:46,556 removing temp file /tmp/tmpmdvfvrgv\n",
      "2019-07-24 00:06:46,808 loading file /hdd1/rini.jannati/.flair/models/imdb.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd1/rini.jannati/anaconda3/lib/python3.6/site-packages/torch/serialization.py:542: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  result = unpickler.load()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEGATIVE (0.9706420302391052)]\n"
     ]
    }
   ],
   "source": [
    "#Tagging with Pre-Trained Text Classification Models\n",
    "from flair.models import TextClassifier\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "sentence = Sentence('This film hurts. It is so bad that I am confused.')\n",
    "classifier.predict(sentence)\n",
    "#print predicted label\n",
    "print(sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 00:16:39,015 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpr118nmqe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000128/160000128 [02:13<00:00, 1199564.43B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 00:18:53,582 copying /tmp/tmpr118nmqe to cache at /hdd1/rini.jannati/.flair/embeddings/glove.gensim.vectors.npy\n",
      "2019-07-24 00:18:53,717 removing temp file /tmp/tmpr118nmqe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 00:18:54,942 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /tmp/tmp1x_58bzg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21494764/21494764 [00:08<00:00, 2404422.63B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 00:19:05,074 copying /tmp/tmp1x_58bzg to cache at /hdd1/rini.jannati/.flair/embeddings/glove.gensim\n",
      "2019-07-24 00:19:05,096 removing temp file /tmp/tmp1x_58bzg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
      "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
      "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
      "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
      "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
      "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
      "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
      "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
      "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
      "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
      "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
      "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
      "        -0.5203, -0.1459,  0.8278,  0.2706])\n",
      "Token: 2 grass\n",
      "tensor([-0.8135,  0.9404, -0.2405, -0.1350,  0.0557,  0.3363,  0.0802, -0.1015,\n",
      "        -0.5478, -0.3537,  0.0734,  0.2587,  0.1987, -0.1433,  0.2507,  0.4281,\n",
      "         0.1950,  0.5346,  0.7424,  0.0578, -0.3178,  0.9436,  0.8145, -0.0824,\n",
      "         0.6166,  0.7284, -0.3262, -1.3641,  0.1232,  0.5373, -0.5123,  0.0246,\n",
      "         1.0822, -0.2296,  0.6039,  0.5541, -0.9610,  0.4803,  0.0022,  0.5591,\n",
      "        -0.1637, -0.8468,  0.0741, -0.6216,  0.0260, -0.5162, -0.0525, -0.1418,\n",
      "        -0.0161, -0.4972, -0.5534, -0.4037,  0.5096,  1.0276, -0.0840, -1.1179,\n",
      "         0.3226,  0.4928,  0.9488,  0.2040,  0.5388,  0.8397, -0.0689,  0.3136,\n",
      "         1.0450, -0.2267, -0.0896, -0.6427,  0.6443, -1.1001, -0.0096,  0.2668,\n",
      "        -0.3230, -0.6065,  0.0479, -0.1664,  0.8571,  0.2335,  0.2539,  1.2546,\n",
      "         0.5472, -0.1980, -0.7186,  0.2076, -0.2587, -0.3650,  0.0834,  0.6932,\n",
      "         0.1574,  1.0931,  0.0913, -1.3773, -0.2717,  0.7071,  0.1872, -0.3307,\n",
      "        -0.2836,  0.1030,  1.2228,  0.8374])\n",
      "Token: 3 is\n",
      "tensor([-0.5426,  0.4148,  1.0322, -0.4024,  0.4669,  0.2182, -0.0749,  0.4733,\n",
      "         0.0810, -0.2208, -0.1281, -0.1144,  0.5089,  0.1157,  0.0282, -0.3628,\n",
      "         0.4382,  0.0475,  0.2028,  0.4986, -0.1007,  0.1327,  0.1697,  0.1165,\n",
      "         0.3135,  0.2571,  0.0928, -0.5683, -0.5297, -0.0515, -0.6733,  0.9253,\n",
      "         0.2693,  0.2273,  0.6636,  0.2622,  0.1972,  0.2609,  0.1877, -0.3454,\n",
      "        -0.4263,  0.1398,  0.5634, -0.5691,  0.1240, -0.1289,  0.7248, -0.2610,\n",
      "        -0.2631, -0.4360,  0.0789, -0.8415,  0.5160,  1.3997, -0.7646, -3.1453,\n",
      "        -0.2920, -0.3125,  1.5129,  0.5243,  0.2146,  0.4245, -0.0884, -0.1780,\n",
      "         1.1876,  0.1058,  0.7657,  0.2191,  0.3582, -0.1164,  0.0933, -0.6248,\n",
      "        -0.2190,  0.2180,  0.7406, -0.4374,  0.1434,  0.1472, -1.1605, -0.0505,\n",
      "         0.1268, -0.0144, -0.9868, -0.0913, -1.2054, -0.1197,  0.0478, -0.5400,\n",
      "         0.5246, -0.7096, -0.3253, -0.1346, -0.4131,  0.3343, -0.0072,  0.3225,\n",
      "        -0.0442, -1.2969,  0.7622,  0.4635])\n",
      "Token: 4 green\n",
      "tensor([-6.7907e-01,  3.4908e-01, -2.3984e-01, -9.9652e-01,  7.3782e-01,\n",
      "        -6.5911e-04,  2.8010e-01,  1.7287e-02, -3.6063e-01,  3.6955e-02,\n",
      "        -4.0395e-01,  2.4092e-02,  2.8958e-01,  4.0497e-01,  6.9992e-01,\n",
      "         2.5269e-01,  8.0350e-01,  4.9370e-02,  1.5562e-01, -6.3286e-03,\n",
      "        -2.9414e-01,  1.4728e-01,  1.8977e-01, -5.1791e-01,  3.6986e-01,\n",
      "         7.4582e-01,  8.2689e-02, -7.2601e-01, -4.0939e-01, -9.7822e-02,\n",
      "        -1.4096e-01,  7.1121e-01,  6.1933e-01, -2.5014e-01,  4.2250e-01,\n",
      "         4.8458e-01, -5.1915e-01,  7.7125e-01,  3.6685e-01,  4.9652e-01,\n",
      "        -4.1298e-02, -1.4683e+00,  2.0038e-01,  1.8591e-01,  4.9860e-02,\n",
      "        -1.7523e-01, -3.5528e-01,  9.4153e-01, -1.1898e-01, -5.1903e-01,\n",
      "        -1.1887e-02, -3.9186e-01, -1.7479e-01,  9.3451e-01, -5.8931e-01,\n",
      "        -2.7701e+00,  3.4522e-01,  8.6533e-01,  1.0808e+00, -1.0291e-01,\n",
      "        -9.1220e-02,  5.5092e-01, -3.9473e-01,  5.3676e-01,  1.0383e+00,\n",
      "        -4.0658e-01,  2.4590e-01, -2.6797e-01, -2.6036e-01, -1.4151e-01,\n",
      "        -1.2022e-01,  1.6234e-01, -7.4320e-01, -6.4728e-01,  4.7133e-02,\n",
      "         5.1642e-01,  1.9898e-01,  2.3919e-01,  1.2550e-01,  2.2471e-01,\n",
      "         8.2613e-01,  7.8328e-02, -5.7020e-01,  2.3934e-02, -1.5410e-01,\n",
      "        -2.5739e-01,  4.1262e-01, -4.6967e-01,  8.7914e-01,  7.2629e-01,\n",
      "         5.3862e-02, -1.1575e+00, -4.7835e-01,  2.0139e-01, -1.0051e+00,\n",
      "         1.1515e-01, -9.6609e-01,  1.2960e-01,  1.8388e-01, -3.0383e-02])\n",
      "Token: 5 .\n",
      "tensor([-0.3398,  0.2094,  0.4635, -0.6479, -0.3838,  0.0380,  0.1713,  0.1598,\n",
      "         0.4662, -0.0192,  0.4148, -0.3435,  0.2687,  0.0446,  0.4213, -0.4103,\n",
      "         0.1546,  0.0222, -0.6465,  0.2526,  0.0431, -0.1945,  0.4652,  0.4565,\n",
      "         0.6859,  0.0913,  0.2188, -0.7035,  0.1679, -0.3508, -0.1263,  0.6638,\n",
      "        -0.2582,  0.0365, -0.1361,  0.4025,  0.1429,  0.3813, -0.1228, -0.4589,\n",
      "        -0.2528, -0.3043, -0.1121, -0.2618, -0.2248, -0.4455,  0.2991, -0.8561,\n",
      "        -0.1450, -0.4909,  0.0083, -0.1749,  0.2752,  1.4401, -0.2124, -2.8435,\n",
      "        -0.2796, -0.4572,  1.6386,  0.7881, -0.5526,  0.6500,  0.0864,  0.3901,\n",
      "         1.0632, -0.3538,  0.4833,  0.3460,  0.8417,  0.0987, -0.2421, -0.2705,\n",
      "         0.0453, -0.4015,  0.1139,  0.0062,  0.0367,  0.0185, -1.0213, -0.2081,\n",
      "         0.6407, -0.0688, -0.5864,  0.3348, -1.1432, -0.1148, -0.2509, -0.4591,\n",
      "        -0.0968, -0.1795, -0.0634, -0.6741, -0.0689,  0.5360, -0.8777,  0.3180,\n",
      "        -0.3924, -0.2339,  0.4730, -0.0288])\n"
     ]
    }
   ],
   "source": [
    "#word embeddings\n",
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "# init embedding\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "# create sentence.\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# embed a sentence using glove.\n",
    "glove_embedding.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 05:23:17,169 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models/common_characters not found in cache, downloading to /tmp/tmp4ole6d99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2887/2887 [00:00<00:00, 1254424.08B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 05:23:18,303 copying /tmp/tmp4ole6d99 to cache at /hdd1/rini.jannati/.flair/datasets/common_characters\n",
      "2019-07-24 05:23:18,305 removing temp file /tmp/tmp4ole6d99\n",
      "Token: 1 The\n",
      "tensor([ 0.2937, -0.0616, -0.0477,  0.0459,  0.0712, -0.0295, -0.0212, -0.0966,\n",
      "        -0.0756,  0.0112,  0.0525,  0.3258,  0.0792, -0.0381,  0.2113,  0.1016,\n",
      "         0.0346,  0.1208, -0.1877, -0.0162, -0.0646, -0.2238, -0.1349, -0.1155,\n",
      "        -0.0605, -0.0144,  0.0300,  0.0913, -0.0715, -0.0444,  0.0691, -0.1411,\n",
      "        -0.0755, -0.0640, -0.0616, -0.0124,  0.1426, -0.2406, -0.1052, -0.2037,\n",
      "        -0.1009, -0.0176,  0.2523,  0.1430, -0.0412,  0.0314, -0.0609, -0.0332,\n",
      "         0.0941, -0.0417], grad_fn=<CatBackward>)\n",
      "Token: 2 grass\n",
      "tensor([ 0.1260, -0.1445, -0.2236,  0.0610, -0.3581, -0.0081,  0.0259, -0.0195,\n",
      "         0.0772, -0.1022,  0.1123,  0.0345,  0.1732, -0.2940,  0.0798,  0.2111,\n",
      "         0.1257,  0.0651,  0.0106,  0.1214, -0.0747, -0.0637, -0.0472, -0.2053,\n",
      "         0.1097, -0.0759, -0.1484,  0.0616, -0.1489, -0.0743, -0.0565,  0.0643,\n",
      "         0.1001, -0.0974,  0.1049, -0.0798, -0.1738, -0.1497,  0.0192,  0.0123,\n",
      "        -0.0935, -0.1442,  0.0600, -0.0397, -0.1978,  0.0144,  0.0207, -0.1904,\n",
      "        -0.0381, -0.1996], grad_fn=<CatBackward>)\n",
      "Token: 3 is\n",
      "tensor([ 0.1919, -0.0297, -0.2050, -0.0410, -0.2432, -0.0158, -0.0921, -0.0624,\n",
      "        -0.0731,  0.0116,  0.1189, -0.0703,  0.1375, -0.1996,  0.0790,  0.0971,\n",
      "         0.0291,  0.1369, -0.1237,  0.0251, -0.0806, -0.0902,  0.0051, -0.0775,\n",
      "        -0.0564, -0.0759, -0.1484,  0.0616, -0.1489, -0.0743, -0.0565,  0.0643,\n",
      "         0.1001, -0.0974,  0.1049, -0.0798, -0.1738, -0.1497,  0.0192,  0.0123,\n",
      "        -0.0935, -0.1442,  0.0600, -0.0397, -0.1978,  0.0144,  0.0207, -0.1904,\n",
      "        -0.0381, -0.1996], grad_fn=<CatBackward>)\n",
      "Token: 4 green\n",
      "tensor([ 0.2157,  0.1405, -0.0674,  0.0128,  0.0117,  0.0200, -0.0428,  0.1270,\n",
      "         0.0977, -0.2063, -0.1175,  0.0358,  0.0649, -0.0225,  0.1304,  0.0154,\n",
      "         0.0884,  0.2542,  0.0238, -0.1053,  0.0957, -0.1173,  0.2551, -0.0398,\n",
      "        -0.2326,  0.1688, -0.1544,  0.0501,  0.0207, -0.0967, -0.1283,  0.0760,\n",
      "         0.0173,  0.0275,  0.1727, -0.0635,  0.0430,  0.1188,  0.1686, -0.0358,\n",
      "         0.0517,  0.0675, -0.0251,  0.1735, -0.2190,  0.0494,  0.3040,  0.0327,\n",
      "         0.0451,  0.1773], grad_fn=<CatBackward>)\n",
      "Token: 5 .\n",
      "tensor([ 0.1189, -0.0240,  0.0483, -0.0522,  0.0276, -0.1884, -0.0849, -0.2739,\n",
      "        -0.1950,  0.1173,  0.0445,  0.0469,  0.0198,  0.0865,  0.0787, -0.1423,\n",
      "         0.1562, -0.0298, -0.1281,  0.1017, -0.1078,  0.0650, -0.1274,  0.0572,\n",
      "        -0.2231,  0.2546, -0.0257,  0.0679,  0.0575, -0.1020, -0.0119, -0.0089,\n",
      "        -0.2823, -0.0620,  0.0020,  0.1353,  0.3540, -0.0542, -0.1145,  0.0576,\n",
      "         0.0890,  0.0111, -0.0456,  0.0107, -0.1411,  0.1221, -0.1631, -0.1787,\n",
      "         0.1909,  0.0630], grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#character embedding\n",
    "from flair.embeddings import CharacterEmbeddings\n",
    "\n",
    "# init embedding\n",
    "embedding = CharacterEmbeddings()\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# embed words in sentence\n",
    "embedding.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs100000.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1987533/1987533 [00:20<00:00, 96160.00B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs100000.d50.w2v.bin.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19357958/19357958 [00:30<00:00, 645243.69B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-0.5856,  0.5523, -0.3354, -0.1171, -0.3433, -0.0332,  0.0166,  1.1833,\n",
      "         0.0681,  0.5751,  0.0159,  0.1526, -1.1126,  0.1577,  0.1971, -0.2337,\n",
      "        -0.0967,  0.0749,  0.4202, -0.0612, -0.1640,  0.4570, -0.5658,  0.0820,\n",
      "         0.3273, -0.2498, -1.2409,  0.2872,  0.2371,  0.3297, -0.0793,  0.0299,\n",
      "        -0.4133, -0.0753, -0.1989, -0.3986, -0.2207,  0.0996,  0.1657, -0.0599,\n",
      "        -0.0053, -0.3639,  0.1914,  0.0638, -1.5809, -0.2949, -0.0881, -0.3183,\n",
      "        -0.0033,  0.0371, -0.5856,  0.5523, -0.3354, -0.1171, -0.3433, -0.0332,\n",
      "         0.0166,  1.1833,  0.0681,  0.5751,  0.0159,  0.1526, -1.1126,  0.1577,\n",
      "         0.1971, -0.2337, -0.0967,  0.0749,  0.4202, -0.0612, -0.1640,  0.4570,\n",
      "        -0.5658,  0.0820,  0.3273, -0.2498, -1.2409,  0.2872,  0.2371,  0.3297,\n",
      "        -0.0793,  0.0299, -0.4133, -0.0753, -0.1989, -0.3986, -0.2207,  0.0996,\n",
      "         0.1657, -0.0599, -0.0053, -0.3639,  0.1914,  0.0638, -1.5809, -0.2949,\n",
      "        -0.0881, -0.3183, -0.0033,  0.0371])\n",
      "Token: 2 grass\n",
      "tensor([ 3.7043e-01, -7.1781e-01, -4.8909e-01,  3.8423e-01,  6.8443e-01,\n",
      "         1.0539e+00,  9.0414e-02,  2.3652e-01, -4.8086e-01,  2.7979e-01,\n",
      "        -1.0836e+00, -1.1437e-01, -2.1852e-01,  6.0321e-01, -2.9981e-01,\n",
      "         7.5256e-01,  1.3014e-02, -5.5774e-01, -1.2554e+00, -1.7130e-01,\n",
      "         6.5900e-04, -1.5592e-01, -7.8476e-02,  6.5893e-01,  8.5455e-01,\n",
      "         8.5906e-01, -7.2761e-01,  1.6983e-01,  1.0549e-01,  7.3701e-02,\n",
      "        -3.9418e-01,  1.0826e+00, -8.1298e-01,  1.6654e-01, -7.7119e-01,\n",
      "         9.6170e-03, -5.1340e-01, -1.2720e-01, -3.4078e-01, -3.9828e-01,\n",
      "         8.4941e-01, -1.9902e-02,  6.0247e-01,  2.2528e-01, -4.6700e-01,\n",
      "         1.6406e-01, -4.8755e-01, -3.6456e-02,  1.0278e+00, -5.5200e-03,\n",
      "         3.7043e-01, -7.1781e-01, -4.8909e-01,  3.8423e-01,  6.8443e-01,\n",
      "         1.0539e+00,  9.0414e-02,  2.3652e-01, -4.8086e-01,  2.7979e-01,\n",
      "        -1.0836e+00, -1.1437e-01, -2.1852e-01,  6.0321e-01, -2.9981e-01,\n",
      "         7.5256e-01,  1.3014e-02, -5.5774e-01, -1.2554e+00, -1.7130e-01,\n",
      "         6.5900e-04, -1.5592e-01, -7.8476e-02,  6.5893e-01,  8.5455e-01,\n",
      "         8.5906e-01, -7.2761e-01,  1.6983e-01,  1.0549e-01,  7.3701e-02,\n",
      "        -3.9418e-01,  1.0826e+00, -8.1298e-01,  1.6654e-01, -7.7119e-01,\n",
      "         9.6170e-03, -5.1340e-01, -1.2720e-01, -3.4078e-01, -3.9828e-01,\n",
      "         8.4941e-01, -1.9902e-02,  6.0247e-01,  2.2528e-01, -4.6700e-01,\n",
      "         1.6406e-01, -4.8755e-01, -3.6456e-02,  1.0278e+00, -5.5200e-03])\n",
      "Token: 3 is\n",
      "tensor([-0.1866,  0.5280, -1.0116,  0.4169, -0.1664, -0.9098,  0.6915,  0.6655,\n",
      "        -0.5491,  0.2236,  0.3772,  0.3319, -0.2402,  0.1800, -0.4039,  0.1565,\n",
      "        -0.3056, -0.0114, -0.1140,  0.5272, -0.1889,  0.1585,  0.0958,  0.9225,\n",
      "        -0.1572, -0.1255, -1.3189,  0.1635,  0.6972,  0.5661, -0.4012,  0.6330,\n",
      "        -0.0752, -0.1147,  0.1681, -0.3840, -0.3728,  0.4368, -0.1956, -0.1691,\n",
      "         0.0704, -0.6942,  0.0227,  0.0618, -1.2336, -0.3819, -0.0040, -0.4201,\n",
      "         0.2203,  0.0800, -0.1866,  0.5280, -1.0116,  0.4169, -0.1664, -0.9098,\n",
      "         0.6915,  0.6655, -0.5491,  0.2236,  0.3772,  0.3319, -0.2402,  0.1800,\n",
      "        -0.4039,  0.1565, -0.3056, -0.0114, -0.1140,  0.5272, -0.1889,  0.1585,\n",
      "         0.0958,  0.9225, -0.1572, -0.1255, -1.3189,  0.1635,  0.6972,  0.5661,\n",
      "        -0.4012,  0.6330, -0.0752, -0.1147,  0.1681, -0.3840, -0.3728,  0.4368,\n",
      "        -0.1956, -0.1691,  0.0704, -0.6942,  0.0227,  0.0618, -1.2336, -0.3819,\n",
      "        -0.0040, -0.4201,  0.2203,  0.0800])\n",
      "Token: 4 green\n",
      "tensor([-0.0755, -0.8742,  0.2042,  1.0616, -0.2461,  0.3552,  0.2628,  0.3877,\n",
      "        -0.5337,  0.2375, -0.5753, -0.2933, -1.3854,  0.4845, -0.3947,  0.4662,\n",
      "        -0.3503,  0.3839, -0.4909,  0.5113,  0.2292, -0.6994, -0.6007,  0.3549,\n",
      "         0.2109,  0.3148, -0.9653,  0.1728, -0.4916, -0.4178, -0.7608,  0.7871,\n",
      "        -0.3039,  0.5810,  0.0154,  0.1105, -0.5488, -0.2559, -0.7226, -0.7317,\n",
      "         0.7235, -0.6333,  0.6838,  0.7554, -1.2394, -0.1344,  0.6778, -0.4609,\n",
      "         0.2638, -0.0036, -0.0755, -0.8742,  0.2042,  1.0616, -0.2461,  0.3552,\n",
      "         0.2628,  0.3877, -0.5337,  0.2375, -0.5753, -0.2933, -1.3854,  0.4845,\n",
      "        -0.3947,  0.4662, -0.3503,  0.3839, -0.4909,  0.5113,  0.2292, -0.6994,\n",
      "        -0.6007,  0.3549,  0.2109,  0.3148, -0.9653,  0.1728, -0.4916, -0.4178,\n",
      "        -0.7608,  0.7871, -0.3039,  0.5810,  0.0154,  0.1105, -0.5488, -0.2559,\n",
      "        -0.7226, -0.7317,  0.7235, -0.6333,  0.6838,  0.7554, -1.2394, -0.1344,\n",
      "         0.6778, -0.4609,  0.2638, -0.0036])\n",
      "Token: 5 .\n",
      "tensor([-0.2147,  0.2122, -0.6071,  0.5129, -0.3256,  0.2208, -0.3712,  1.1548,\n",
      "        -0.0595,  0.3382,  0.1074, -0.3109, -0.7043, -0.5039, -0.3101,  0.1500,\n",
      "         0.0888,  0.6306, -0.6351,  0.0741, -0.3744, -0.1120, -0.1191,  0.1893,\n",
      "         0.3136,  0.0117, -0.6044,  0.3977,  0.7467, -0.5820, -0.0789,  0.1626,\n",
      "         0.3100, -0.0591,  0.8583, -0.1298, -0.2150,  0.3992, -0.8243,  0.7244,\n",
      "        -0.4512, -0.3404, -0.3065,  0.0288, -0.8848,  0.5191,  0.2391,  0.6826,\n",
      "         0.2182,  0.4763, -0.2147,  0.2122, -0.6071,  0.5129, -0.3256,  0.2208,\n",
      "        -0.3712,  1.1548, -0.0595,  0.3382,  0.1074, -0.3109, -0.7043, -0.5039,\n",
      "        -0.3101,  0.1500,  0.0888,  0.6306, -0.6351,  0.0741, -0.3744, -0.1120,\n",
      "        -0.1191,  0.1893,  0.3136,  0.0117, -0.6044,  0.3977,  0.7467, -0.5820,\n",
      "        -0.0789,  0.1626,  0.3100, -0.0591,  0.8583, -0.1298, -0.2150,  0.3992,\n",
      "        -0.8243,  0.7244, -0.4512, -0.3404, -0.3065,  0.0288, -0.8848,  0.5191,\n",
      "         0.2391,  0.6826,  0.2182,  0.4763])\n"
     ]
    }
   ],
   "source": [
    "#byte embeddings\n",
    "from flair.embeddings import BytePairEmbeddings\n",
    "\n",
    "# init embedding\n",
    "embedding = BytePairEmbeddings('en')\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# embed words in sentence\n",
    "embedding.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 05:39:56,040 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-backward--h2048-l1-d0.05-lr30-0.25-20/news-backward-0.4.1.pt not found in cache, downloading to /tmp/tmpl3o3h36n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73034575/73034575 [00:50<00:00, 1438632.96B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 05:40:48,022 copying /tmp/tmpl3o3h36n to cache at /hdd1/rini.jannati/.flair/embeddings/news-backward-0.4.1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 05:40:48,088 removing temp file /tmp/tmpl3o3h36n\n",
      "Token: 1 The\n",
      "tensor([ 0.0085, -0.0139, -0.0008,  ..., -0.0065, -0.0053,  0.0090])\n",
      "Token: 2 grass\n",
      "tensor([ 0.0049, -0.0203,  0.0007,  ...,  0.0354, -0.0255, -0.0143])\n",
      "Token: 3 is\n",
      "tensor([ 0.0045,  0.0119, -0.0011,  ..., -0.0005, -0.0097, -0.0275])\n",
      "Token: 4 green\n",
      "tensor([-0.0012, -0.0028,  0.0070,  ..., -0.0007, -0.1333,  0.0161])\n",
      "Token: 5 .\n",
      "tensor([-0.0008, -0.0064, -0.0006,  ...,  0.0005, -0.0177,  0.0032])\n"
     ]
    }
   ],
   "source": [
    "#flair embeddings\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "\n",
    "#init embedding\n",
    "flair_embedding_forward = FlairEmbeddings('news-backward')\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "flair_embedding_forward.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-3.8194e-02, -2.4487e-01,  7.2812e-01,  ..., -4.4014e-04,\n",
      "        -3.9301e-02,  1.0601e-02])\n",
      "Token: 2 grass\n",
      "tensor([-8.1353e-01,  9.4042e-01, -2.4048e-01,  ..., -3.7749e-04,\n",
      "        -2.3563e-02,  1.1700e-02])\n",
      "Token: 3 is\n",
      "tensor([-0.5426,  0.4148,  1.0322,  ..., -0.0061,  0.0112,  0.0100])\n",
      "Token: 4 green\n",
      "tensor([-0.6791,  0.3491, -0.2398,  ..., -0.0026, -0.0118,  0.0455])\n",
      "Token: 5 .\n",
      "tensor([-3.3979e-01,  2.0941e-01,  4.6348e-01,  ..., -2.3405e-04,\n",
      "         3.8688e-03,  5.7725e-03])\n"
     ]
    }
   ],
   "source": [
    "#Recomended Flair Usage\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
    "\n",
    "# create a StackedEmbedding object that combines glove and forward/backward flair embeddings\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "                                        WordEmbeddings('glove'), \n",
    "                                        FlairEmbeddings('news-forward'), \n",
    "                                        FlairEmbeddings('news-backward'),\n",
    "                                       ])\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:01<00:00, 208136.83B/s]\n",
      "100%|██████████| 407873900/407873900 [01:39<00:00, 4109686.14B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-0.0323, -0.3904, -1.1946,  ...,  0.1305, -0.1365, -0.4323])\n",
      "Token: 2 grass\n",
      "tensor([-0.3973,  0.2652, -0.1337,  ...,  0.3715,  0.1097, -1.1625])\n",
      "Token: 3 is\n",
      "tensor([ 0.1374, -0.3688, -0.8292,  ...,  0.2533,  0.0294,  0.4293])\n",
      "Token: 4 green\n",
      "tensor([-0.7722, -0.1152,  0.3661,  ...,  0.1575, -0.0682, -0.7661])\n",
      "Token: 5 .\n",
      "tensor([ 0.1441, -0.1772, -0.5911,  ..., -1.4830,  0.1995, -0.0112])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import BertEmbeddings\n",
    "\n",
    "# init embedding\n",
    "embedding = BertEmbeddings()\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# embed words in sentence\n",
    "embedding.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 05:50:04,338 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-forward-v0.1.pt not found in cache, downloading to /tmp/tmp7wgg0ozf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73034300/73034300 [02:39<00:00, 456499.61B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 05:52:45,583 copying /tmp/tmp7wgg0ozf to cache at /hdd1/rini.jannati/.flair/embeddings/lm-multi-forward-v0.1.pt\n",
      "2019-07-24 05:52:45,648 removing temp file /tmp/tmp7wgg0ozf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 05:52:47,135 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-backward-v0.1.pt not found in cache, downloading to /tmp/tmp8dxusoar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73034304/73034304 [00:57<00:00, 1266678.52B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 05:53:45,984 copying /tmp/tmp8dxusoar to cache at /hdd1/rini.jannati/.flair/embeddings/lm-multi-backward-v0.1.pt\n",
      "2019-07-24 05:53:46,047 removing temp file /tmp/tmp8dxusoar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-24 05:53:46,266 The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 995526/995526 [00:01<00:00, 629498.53B/s]\n",
      "100%|██████████| 662804195/662804195 [02:13<00:00, 4948747.34B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-1.4812e-07,  4.5007e-08,  6.0273e-07,  ...,  3.8287e-01,\n",
      "         4.7210e-01,  2.9850e-01])\n",
      "Token: 2 grass\n",
      "tensor([ 1.6254e-04,  1.8764e-07, -7.9040e-09,  ...,  8.5283e-01,\n",
      "        -5.0724e-02,  3.4476e-01])\n",
      "Token: 3 is\n",
      "tensor([-2.4521e-04,  3.4869e-07,  5.5841e-06,  ..., -1.8283e-01,\n",
      "         7.1532e-01,  5.0834e-03])\n",
      "Token: 4 green\n",
      "tensor([8.3005e-05, 4.7261e-08, 5.7315e-07,  ..., 1.0157e+00, 7.5358e-01,\n",
      "        1.1230e-01])\n",
      "Token: 5 .\n",
      "tensor([-8.3244e-07,  1.6451e-07, -1.7201e-08,  ..., -6.0930e-01,\n",
      "         9.0591e-01,  1.7857e-01])\n"
     ]
    }
   ],
   "source": [
    "#combining BERT and Flair\n",
    "# init Flair embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('multi-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('multi-backward')\n",
    "\n",
    "# init multilingual BERT\n",
    "bert_embedding = BertEmbeddings('bert-base-multilingual-cased')\n",
    "\n",
    "# now create the StackedEmbedding object that combines all embeddings\n",
    "stacked_embeddings = StackedEmbeddings(\n",
    "    embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding])\n",
    "\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3197,  0.2621,  0.4037,  ..., -0.0013, -0.0026,  0.0170],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "#document embedding\n",
    "from flair.embeddings import DocumentPoolEmbeddings\n",
    "\n",
    "\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('news-backward')\n",
    "\n",
    "# initialize the document embeddings, mode = mean\n",
    "document_embeddings = DocumentPoolEmbeddings([glove_embedding,\n",
    "                                              flair_embedding_backward,\n",
    "                                              flair_embedding_forward])\n",
    "# create an example sentence\n",
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "# embed the sentence with our document embedding\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3611, -0.4075, -0.3058,  0.0844,  0.4464, -0.1202,  0.1306, -0.0170,\n",
      "        -0.1114, -0.0016,  0.1565,  0.1655,  0.1841,  0.1598, -0.0366,  0.1081,\n",
      "        -0.3431,  0.1110,  0.3392, -0.0550,  0.0877,  0.1476, -0.2228,  0.0507,\n",
      "         0.3188,  0.1978,  0.0557,  0.1017, -0.4407, -0.0708, -0.1799,  0.3755,\n",
      "         0.5589,  0.2389, -0.0745,  0.3494, -0.0009,  0.1357, -0.2437, -0.0323,\n",
      "         0.1094,  0.0826,  0.3017, -0.2307, -0.0368,  0.0149, -0.2141, -0.0688,\n",
      "         0.0704,  0.1382,  0.2682,  0.0102,  0.2276,  0.4115, -0.0642, -0.1709,\n",
      "        -0.2537,  0.2186,  0.0820, -0.0866,  0.1253,  0.1713,  0.1156, -0.2100,\n",
      "        -0.0707, -0.2387,  0.0403,  0.1954,  0.0179,  0.3141, -0.0201, -0.1996,\n",
      "        -0.1646,  0.0878, -0.0858, -0.0019,  0.1602,  0.0915, -0.0377, -0.0459,\n",
      "        -0.0044, -0.0026,  0.1438,  0.3546, -0.0307, -0.2299, -0.2442, -0.3343,\n",
      "        -0.1303,  0.0075, -0.1675,  0.1826, -0.2145,  0.0438,  0.1381, -0.2730,\n",
      "         0.2075, -0.1589, -0.2021, -0.1773], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "# instantiate pre-trained word embeddings\n",
    "embeddings = WordEmbeddings('glove')\n",
    "\n",
    "# document pool embeddings\n",
    "document_embeddings = DocumentPoolEmbeddings([embeddings], fine_tune_mode='nonlinear')\n",
    "\n",
    "# create an example sentence\n",
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "# embed the sentence with our document embedding\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  0.0000, -0.4553,  0.4505,  0.4919,  0.0000,  0.0000, -0.2491,\n",
      "         0.0199,  0.0000, -0.0360,  0.6161, -0.2105,  0.0778, -0.1140,  0.0000,\n",
      "         0.0000,  0.0000,  0.2780, -0.4031,  0.0000,  0.0612,  0.0000,  0.1315,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000, -0.1700, -0.4406,  0.9832,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.3916,  0.0000, -0.0943,  0.2787,  0.5026,\n",
      "         0.0000,  0.0205,  0.0000, -0.1090,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.2306,  0.2194,  0.7100,  0.0103,  0.3488, -0.0263,  0.0000,  0.0000,\n",
      "         0.0000, -0.6434,  0.0000,  0.0000,  0.0000,  0.0000, -0.0185,  0.0000,\n",
      "         0.0000,  0.0000,  0.0829,  0.0000,  0.0000,  0.0000,  0.0000,  0.3702,\n",
      "         0.0000,  0.1944,  0.1992,  0.0000,  0.6486,  0.2896, -0.7297,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2458, -0.9583,  0.5178,\n",
      "         0.0000,  0.4080,  0.0000,  0.8901,  0.0000,  0.4160, -0.8192,  0.0000,\n",
      "         0.0000,  0.0000, -0.9124, -0.5000,  0.0000,  0.0061,  0.0000, -0.4577,\n",
      "         0.0000,  0.0000,  0.3733,  0.0000,  0.0000,  0.1131,  0.0000,  0.0000,\n",
      "         0.2298,  0.4482, -0.2134,  0.2526, -0.1583, -1.0001, -0.0846,  0.0000,\n",
      "         0.0000,  0.0000,  0.2910,  0.0000,  0.0000, -0.2093,  0.0000,  0.0000],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "#DocumentRNNEmbedings\n",
    "from flair.embeddings import DocumentRNNEmbeddings\n",
    "\n",
    "\n",
    "document_rnn_embeddings = DocumentRNNEmbeddings([glove_embedding])\n",
    "\n",
    "# create an example sentence\n",
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "# embed the sentence with our document embedding\n",
    "document_rnn_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0817,  0.0521, -0.0149,  ..., -0.2123,  0.0226, -0.0833],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "document_lstm_embeddings = DocumentRNNEmbeddings([glove_embedding], rnn_type='LSTM')\n",
    "document_embeddings = classifier.document_embeddings\n",
    "\n",
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
